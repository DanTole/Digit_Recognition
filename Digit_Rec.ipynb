{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./data/train.csv', delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [7., 0., 0., ..., 0., 0., 0.],\n",
       "       [6., 0., 0., ..., 0., 0., 0.],\n",
       "       [9., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the images looks like ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImage(index, X, Y):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    image = X.reshape(28,28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f'Index: {index} \\nLabel : {int(Y[index])}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = np.random.choice(range(X.shape[0]), size=1)\n",
    "\n",
    "plotImage(index[0], X[:,index], Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParams(layers_dim):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for index in range(1, len(layers_dim)):\n",
    "        params['W'+str(index)] = np.random.rand(layers_dim[index], layers_dim[index-1])\n",
    "        params['b'+str(index)] = np.zeros((layers_dim[index-1], 1))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1+ np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return z * (z > 0)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation.lower() == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    elif activation.lower() == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    elif activation.lower() == 'tanh':\n",
    "        A = tanh(Z)\n",
    "        \n",
    "    elif activation.lower() == 'softmax':\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    cache = []\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A = linear_activation_forward(A, parameters['W'+str(l)], parameters['b'+str(l)], 'relu')\n",
    "        \n",
    "    AL = linear_activation_forward(A, parameters['W'+str(l)], parameters['b'+str(l)], 'softmax')\n",
    "    \n",
    "    return AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "CrossEntropyCost = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log \\left(a_k^{[L](i)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(Y)\n",
    "    K = AL.shape[0]\n",
    "    \n",
    "    cost = -1/len(m)*np.sum(np.sum(Y*np.log(AL), axis=0))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    assert cost.shape == ()\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation parts TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
